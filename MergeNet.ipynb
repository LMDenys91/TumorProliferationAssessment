{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from keras.layers import merge\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Nadam\n",
    "from keras.metrics import MSE\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ProgbarLogger, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from scipy.stats import spearmanr\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import scandir\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda-7.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img(img_path):\n",
    "    '''This function returns a preprocessed image\n",
    "    '''\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    img = load_img(img_path, target_size=(512, 512))\n",
    "    img = img_to_array(img, dim_ordering=dim_ordering)\n",
    "\n",
    "    if dim_ordering == 'th':\n",
    "        img = img[::-1, :, :]\n",
    "    else:\n",
    "        img = img[:, :, ::-1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_patchblock(inp, idx):\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        inp_shape=(512, 512, 3)\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        inp_shape=shape=(3, 512, 512)\n",
    "        bn_axis = 1\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "        \n",
    "    with tf.device('/gpu:0'):\n",
    "        out = Convolution2D(32, 7, 7, subsample=(2, 2),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv1_{0}'.format(idx), input_shape=(inp_shape))(inp)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv1_{0}'.format(idx))(out)\n",
    "        out = Activation('relu')(out) LeakyReLU(alpha=0.5)\n",
    "        out = MaxPooling2D((3, 3), strides=(2, 2), dim_ordering=dim_ordering)(out)\n",
    "        \n",
    "        out = Convolution2D(32, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv2_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv2_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        out = Convolution2D(32, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv3_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv3_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        out = MaxPooling2D((3, 3), strides=(2, 2), dim_ordering=dim_ordering)(out)\n",
    "        \n",
    "        out = Convolution2D(64, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv4_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv4_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "    with tf.device('/gpu:1'):\n",
    "        out = Convolution2D(64, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv5_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv5_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        out = MaxPooling2D((3, 3), strides=(2, 2), dim_ordering=dim_ordering)(out)\n",
    "\n",
    "        out = Convolution2D(128, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv6_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv6_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        out = Convolution2D(128, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv7_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv7_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        \n",
    "    with tf.device('/gpu:2'):\n",
    "        out = Convolution2D(128, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv8_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv8_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        out = Convolution2D(128, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv9_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv9_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        out = MaxPooling2D((3, 3), strides=(2, 2), dim_ordering=dim_ordering)(out)\n",
    "\n",
    "        out = Convolution2D(256, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv10_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv10_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "    with tf.device('/gpu:3'):\n",
    "        out = Convolution2D(256, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv11_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv11_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        out = Convolution2D(256, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv12_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv12_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        \n",
    "        out = Convolution2D(256, 3, 3, subsample=(1, 1),\n",
    "                            init='he_normal', border_mode='same', dim_ordering=dim_ordering,\n",
    "                            name='conv13_{0}'.format(idx), input_shape=(inp_shape))(out)\n",
    "        out = BatchNormalization(axis=bn_axis, name='bn_conv13_{0}'.format(idx))(out)\n",
    "        out = Activation(LeakyReLU(alpha=0.5))(out)\n",
    "        out = MaxPooling2D((3, 3), strides=(2, 2), dim_ordering=dim_ordering)(out)\n",
    "        out = MaxPooling2D((7, 7), dim_ordering=dim_ordering)(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patches_per_slice = 5\n",
    "def get_mergenet():    \n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        inp_shape=(512, 512, 3)\n",
    "        concat_axis = 3\n",
    "    else:\n",
    "        inp_shape=(3, 512, 512)\n",
    "        concat_axis = 1\n",
    "    \n",
    "    patch_nets = []\n",
    "    input_list = []\n",
    "    for i in range(patches_per_slice):\n",
    "        inp = Input(inp_shape)\n",
    "        model = get_patchblock(inp, i)\n",
    "        patch_nets.append(model)\n",
    "        input_list.append(inp)\n",
    "        \n",
    "    out = merge(patch_nets, mode='concat', concat_axis=concat_axis)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(2, activation='softmax', name='fc')(out)\n",
    "    return Model(input_list, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Started to build model at {0}'.format(datetime.datetime.utcnow()))\n",
    "model = get_mergenet()\n",
    "model.summary()\n",
    "print('Finished building model at {0}'.format(datetime.datetime.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: insert my own loss and metrics functions (one per output), equal to the ones specified by TUPAC\n",
    "print('Started to compile model at {0}'.format(datetime.datetime.utcnow()))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "print('Finished compiling model at {0}'.format(datetime.datetime.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(dir_path, res = 512):\n",
    "    annotations = open('../annotations/training_ground_truth.csv', 'r')\n",
    "    lines = annotations.readlines()\n",
    "    images = []\n",
    "    labels = []\n",
    "    for subdir, _, files in scandir.walk(dir_path):\n",
    "        subdir = subdir.replace('\\\\', '/')  # windows path fix\n",
    "        subdir_split = subdir.split('/')\n",
    "        if len(subdir_split[3])>0: study_id = int(subdir_split[3].lstrip(\"0\"))\n",
    "        else: continue\n",
    "        label = [float(lines[study_id].split(',')[0]), float(lines[study_id].split(',')[1])]\n",
    "        imgs = []\n",
    "        for f in files:\n",
    "            tiff_path = os.path.join(subdir, f)\n",
    "            if not tiff_path.endswith('.tiff'):\n",
    "                continue\n",
    "            img = read_img(tiff_path)\n",
    "            imgs.append(img)\n",
    "        images.append(imgs)\n",
    "        labels.append(label)\n",
    "    annotations.close()\n",
    "    images_list = []\n",
    "    print(np.asarray(images))\n",
    "    for x in np.split(np.swapaxes(images, 0, 1), 5):\n",
    "        images_list.append(np.squeeze(x))\n",
    "    return images_list, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "images_train, labels_train = get_data('../example_images/train/')\n",
    "images_val, labels_val = get_data('../example_images/val/')\n",
    "print(images_train[0].shape[3]!=512, len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# featurewise standardization, normalization and augmentation (horizontal and vertical flips)\n",
    "def augment_data(X):\n",
    "    X = np.asarray(X).swapaxes(1, 0)\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X -= mean\n",
    "    std = np.std(X, axis=0)\n",
    "    X /= (std + 1e-7)\n",
    "    X = X.swapaxes(0, 1)\n",
    "    if np.random.random() < 0.5:\n",
    "        X = flip_axis(X, 2)\n",
    "    if np.random.random() < 0.5:\n",
    "        X = flip_axis(X, 3)\n",
    "    return X\n",
    "        \n",
    "def flip_axis(X, axis):\n",
    "    X = np.asarray(X).swapaxes(axis, 0)\n",
    "    X = X[::-1, ...]\n",
    "    X = X.swapaxes(0, axis)\n",
    "    return X\n",
    "\n",
    "print('Started data augmentation at {0}'.format(datetime.datetime.utcnow()))\n",
    "images_train = augment_data(images_train)\n",
    "images_val = augment_data(images_val)\n",
    "print('Finished data augmentation at {0}'.format(datetime.datetime.utcnow()))\n",
    "#print(np.mean(images_train[0]), np.std(images_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "nb_epoch = 10\n",
    "patches_per_slice = 5\n",
    "#class_weights = {}\n",
    "callbacks = [ProgbarLogger(),\n",
    "             ModelCheckpoint('mergenet_weights.hdf5', monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, mode='auto')]\n",
    "\n",
    "print('Started fitting at {0}'.format(datetime.datetime.utcnow()))\n",
    "history = model.fit(images_train, labels_train, batch_size,\n",
    "                              #samples_per_epoch=images_train.shape[0],\n",
    "                              #class_weights = class_weights,\n",
    "                              nb_epoch=nb_epoch, verbose=1,\n",
    "                              validation_data=(images_val, labels_val),\n",
    "                              #nb_val_samples = 1,\n",
    "                              callbacks=callbacks)\n",
    "print('Finished fitting at {0}'.format(datetime.datetime.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decode predictions to one hot encoding\n",
    "# Use ranking approach from https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md\n",
    "# The predicted values are mapped to classes based on the CDF of the ref values.\n",
    "hist = np.bincount(labels_train[:,0].astype(int))\n",
    "cdf = np.cumsum(hist) / float(sum(hist))\n",
    "np.savetxt(\"../example_images/train/cdf.txt\", cdf)\n",
    "\n",
    "def getScore(pred, cdf, valid=False):\n",
    "    num = pred.shape[0]\n",
    "    output = np.asarray([4]*num, dtype=int)\n",
    "    rank = pred.argsort()\n",
    "    output[rank[:int(num*cdf[0]-1)]] = 1\n",
    "    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 2\n",
    "    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 3\n",
    "    if valid:\n",
    "        cutoff = [ pred[rank[int(num*cdf[i]-1)]] for i in range(3) ]\n",
    "        return output, cutoff\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Metrics on validation set:\\n------------------------------------------------------------\\n')\n",
    "pred = model.predict(images_val, verbose=0)\n",
    "pred_class = getScore(pred[:,0], cdf)\n",
    "print('Confusion Matrix:\\n', confusion_matrix(labels_val[:,0], pred_class), '\\n')\n",
    "print(classification_report(labels_val[:,0], pred_class), '\\n')\n",
    "print(\"Quadratic Weighted Cohen's Kappa: \", quadratic_weighted_kappa(pred[:,0], labels_val[:,0]))\n",
    "print(\"Spearman's Correlation CoÃ«fficient: \", spearmanr(pred[:,1], labels_val[:,1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
